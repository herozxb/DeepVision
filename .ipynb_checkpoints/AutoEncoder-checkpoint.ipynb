{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'metrics'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-93d3768d9cc5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitializers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mVarianceScaling\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcluster\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mKMeans\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__import__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'sys'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named 'metrics'"
     ]
    }
   ],
   "source": [
    "#test the litter set\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score #works\n",
    "from sklearn.cluster import KMeans\n",
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "from time import time\n",
    "import numpy as np\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.layers import Dense, Input,Layer, InputSpec\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras import callbacks\n",
    "from tensorflow.keras.initializers import VarianceScaling\n",
    "from sklearn.cluster import KMeans\n",
    "import metrics\n",
    "\n",
    "print(__import__('sys').version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mnist' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-f39c2b92ed56>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmnist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdivide\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m255.\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'mnist' is not defined"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x = np.concatenate((x_train, x_test))\n",
    "y = np.concatenate((y_train, y_test))\n",
    "x = x.reshape((x.shape[0], -1))\n",
    "x = np.divide(x, 255.)\n",
    "# 10 clusters\n",
    "n_clusters = len(np.unique(y))\n",
    "# Runs in parallel 4 CPUs\n",
    "kmeans = KMeans(n_clusters=n_clusters, n_init=20, n_jobs=4)\n",
    "# Train K-Means.\n",
    "y_pred_kmeans = kmeans.fit_predict(x)\n",
    "# Evaluate the K-Means clustering accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accuracy_score(y, y_pred_kmeans))\n",
    "metrics.acc(y, y_pred_kmeans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def autoencoder(dims, act='relu', init='glorot_uniform'):\n",
    "    \"\"\"\n",
    "    Fully connected auto-encoder model, symmetric.\n",
    "    Arguments:\n",
    "        dims: list of number of units in each layer of encoder. dims[0] is input dim, dims[-1] is units in hidden layer.\n",
    "            The decoder is symmetric with encoder. So number of layers of the auto-encoder is 2*len(dims)-1\n",
    "        act: activation, not applied to Input, Hidden and Output layers\n",
    "    return:\n",
    "        (ae_model, encoder_model), Model of autoencoder and model of encoder\n",
    "    \"\"\"\n",
    "    n_stacks = len(dims) - 1\n",
    "    # input\n",
    "    input_img = Input(shape=(dims[0],), name='input')\n",
    "    x = input_img\n",
    "    # internal layers in encoder\n",
    "    for i in range(n_stacks-1):\n",
    "        x = Dense(dims[i + 1], activation=act, kernel_initializer=init, name='encoder_%d' % i)(x)\n",
    "\n",
    "    # hidden layer\n",
    "    encoded = Dense(dims[-1], kernel_initializer=init, name='encoder_%d' % (n_stacks - 1))(x)  # hidden layer, features are extracted from here\n",
    "\n",
    "    x = encoded\n",
    "    # internal layers in decoder\n",
    "    for i in range(n_stacks-1, 0, -1):\n",
    "        x = Dense(dims[i], activation=act, kernel_initializer=init, name='decoder_%d' % i)(x)\n",
    "\n",
    "    # output\n",
    "    x = Dense(dims[0], kernel_initializer=init, name='decoder_0')(x)\n",
    "    decoded = x\n",
    "    return Model(inputs=input_img, outputs=decoded, name='AE'), Model(inputs=input_img, outputs=encoded, name='encoder')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "(10000, 28, 28)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(70000, 784)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)\n",
    "x = np.concatenate((x_train, x_test))\n",
    "y = np.concatenate((y_train, y_test))\n",
    "x = x.reshape((x.shape[0], -1))\n",
    "x = np.divide(x, 255.)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(70000, 784)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_clusters = len(np.unique(y))\n",
    "print(n_clusters)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/king/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "dims = [x.shape[-1], 500, 500, 2000, 10]\n",
    "init = VarianceScaling(scale=1. / 3., mode='fan_in',\n",
    "                           distribution='uniform')\n",
    "pretrain_optimizer = SGD(lr=1, momentum=0.9)\n",
    "pretrain_epochs = 300\n",
    "batch_size = 256\n",
    "save_dir = './results'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder, encoder = autoencoder(dims, init=init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from tensorflow.keras.utils import plot_model\n",
    "#plot_model(autoencoder, to_file='autoencoder.png', show_shapes=True)\n",
    "#from IPython.display import Image\n",
    "#Image(filename='autoencoder.png') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/king/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/utils/losses_utils.py:170: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /home/king/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/300\n",
      "70000/70000 [==============================] - 20s 286us/sample - loss: 0.0646\n",
      "Epoch 2/300\n",
      "70000/70000 [==============================] - 1s 16us/sample - loss: 0.0436\n",
      "Epoch 3/300\n",
      "70000/70000 [==============================] - 1s 17us/sample - loss: 0.0318\n",
      "Epoch 4/300\n",
      "70000/70000 [==============================] - 1s 15us/sample - loss: 0.0266\n",
      "Epoch 5/300\n",
      "70000/70000 [==============================] - 1s 17us/sample - loss: 0.0239\n",
      "Epoch 6/300\n",
      "70000/70000 [==============================] - 1s 17us/sample - loss: 0.0223\n",
      "Epoch 7/300\n",
      "70000/70000 [==============================] - 1s 16us/sample - loss: 0.0211\n",
      "Epoch 8/300\n",
      "70000/70000 [==============================] - 1s 16us/sample - loss: 0.0202\n",
      "Epoch 9/300\n",
      "70000/70000 [==============================] - 1s 18us/sample - loss: 0.0195\n",
      "Epoch 10/300\n",
      "70000/70000 [==============================] - 2s 22us/sample - loss: 0.0189\n",
      "Epoch 11/300\n",
      "70000/70000 [==============================] - 1s 17us/sample - loss: 0.0184\n",
      "Epoch 12/300\n",
      "70000/70000 [==============================] - 1s 18us/sample - loss: 0.0179\n",
      "Epoch 13/300\n",
      "70000/70000 [==============================] - 1s 17us/sample - loss: 0.0175\n",
      "Epoch 14/300\n",
      "70000/70000 [==============================] - 1s 17us/sample - loss: 0.0171\n",
      "Epoch 15/300\n",
      "70000/70000 [==============================] - 1s 18us/sample - loss: 0.0168\n",
      "Epoch 16/300\n",
      "70000/70000 [==============================] - 1s 17us/sample - loss: 0.0165\n",
      "Epoch 17/300\n",
      "70000/70000 [==============================] - 1s 15us/sample - loss: 0.0163\n",
      "Epoch 18/300\n",
      "70000/70000 [==============================] - 1s 15us/sample - loss: 0.0160\n",
      "Epoch 19/300\n",
      "70000/70000 [==============================] - 1s 15us/sample - loss: 0.0158\n",
      "Epoch 20/300\n",
      "70000/70000 [==============================] - 1s 15us/sample - loss: 0.0156\n",
      "Epoch 21/300\n",
      "70000/70000 [==============================] - 1s 15us/sample - loss: 0.0154\n",
      "Epoch 22/300\n",
      "70000/70000 [==============================] - 1s 15us/sample - loss: 0.0152\n",
      "Epoch 23/300\n",
      "70000/70000 [==============================] - 1s 14us/sample - loss: 0.0150\n",
      "Epoch 24/300\n",
      "70000/70000 [==============================] - 1s 14us/sample - loss: 0.0149\n",
      "Epoch 25/300\n",
      "70000/70000 [==============================] - 1s 14us/sample - loss: 0.0147\n",
      "Epoch 26/300\n",
      "70000/70000 [==============================] - 1s 14us/sample - loss: 0.0146\n",
      "Epoch 27/300\n",
      "70000/70000 [==============================] - 1s 15us/sample - loss: 0.0145\n",
      "Epoch 28/300\n",
      "70000/70000 [==============================] - 1s 14us/sample - loss: 0.0143\n",
      "Epoch 29/300\n",
      "70000/70000 [==============================] - 1s 14us/sample - loss: 0.0142\n",
      "Epoch 30/300\n",
      "70000/70000 [==============================] - 1s 14us/sample - loss: 0.0141\n",
      "Epoch 31/300\n",
      "70000/70000 [==============================] - 1s 14us/sample - loss: 0.0140\n",
      "Epoch 32/300\n",
      "70000/70000 [==============================] - 1s 15us/sample - loss: 0.0139\n",
      "Epoch 33/300\n",
      "70000/70000 [==============================] - 1s 15us/sample - loss: 0.0138\n",
      "Epoch 34/300\n",
      "70000/70000 [==============================] - 1s 15us/sample - loss: 0.0137\n",
      "Epoch 35/300\n",
      "70000/70000 [==============================] - 2s 22us/sample - loss: 0.0136\n",
      "Epoch 36/300\n",
      "70000/70000 [==============================] - 2s 26us/sample - loss: 0.0135\n",
      "Epoch 37/300\n",
      "70000/70000 [==============================] - 2s 32us/sample - loss: 0.0134\n",
      "Epoch 38/300\n",
      "70000/70000 [==============================] - 3s 36us/sample - loss: 0.0133\n",
      "Epoch 39/300\n",
      "70000/70000 [==============================] - 2s 30us/sample - loss: 0.0132\n",
      "Epoch 40/300\n",
      "70000/70000 [==============================] - 2s 31us/sample - loss: 0.0131\n",
      "Epoch 41/300\n",
      "70000/70000 [==============================] - 2s 29us/sample - loss: 0.0131\n",
      "Epoch 42/300\n",
      "70000/70000 [==============================] - 2s 26us/sample - loss: 0.0130\n",
      "Epoch 43/300\n",
      "70000/70000 [==============================] - 2s 25us/sample - loss: 0.0129\n",
      "Epoch 44/300\n",
      "70000/70000 [==============================] - 2s 23us/sample - loss: 0.0128\n",
      "Epoch 45/300\n",
      "70000/70000 [==============================] - 1s 20us/sample - loss: 0.0128\n",
      "Epoch 46/300\n",
      "70000/70000 [==============================] - 1s 19us/sample - loss: 0.0127\n",
      "Epoch 47/300\n",
      "70000/70000 [==============================] - 1s 16us/sample - loss: 0.0126\n",
      "Epoch 48/300\n",
      "70000/70000 [==============================] - 1s 16us/sample - loss: 0.0126\n",
      "Epoch 49/300\n",
      "70000/70000 [==============================] - 1s 16us/sample - loss: 0.0125\n",
      "Epoch 50/300\n",
      "70000/70000 [==============================] - 1s 16us/sample - loss: 0.0124\n",
      "Epoch 51/300\n",
      "70000/70000 [==============================] - 1s 16us/sample - loss: 0.0124\n",
      "Epoch 52/300\n",
      "70000/70000 [==============================] - 1s 16us/sample - loss: 0.0123\n",
      "Epoch 53/300\n",
      "70000/70000 [==============================] - 1s 15us/sample - loss: 0.0123\n",
      "Epoch 54/300\n",
      "70000/70000 [==============================] - 1s 15us/sample - loss: 0.0122\n",
      "Epoch 55/300\n",
      "70000/70000 [==============================] - 1s 15us/sample - loss: 0.0122\n",
      "Epoch 56/300\n",
      "70000/70000 [==============================] - 1s 15us/sample - loss: 0.0121\n",
      "Epoch 57/300\n",
      "70000/70000 [==============================] - 1s 15us/sample - loss: 0.0121\n",
      "Epoch 58/300\n",
      "70000/70000 [==============================] - 1s 15us/sample - loss: 0.0120\n",
      "Epoch 59/300\n",
      "70000/70000 [==============================] - 1s 15us/sample - loss: 0.0120\n",
      "Epoch 60/300\n",
      "70000/70000 [==============================] - 1s 15us/sample - loss: 0.0119\n",
      "Epoch 61/300\n",
      "70000/70000 [==============================] - 1s 15us/sample - loss: 0.0119\n",
      "Epoch 62/300\n",
      "70000/70000 [==============================] - 1s 15us/sample - loss: 0.0118\n",
      "Epoch 63/300\n",
      "70000/70000 [==============================] - 1s 15us/sample - loss: 0.0118\n",
      "Epoch 64/300\n",
      "70000/70000 [==============================] - 1s 15us/sample - loss: 0.0117\n",
      "Epoch 65/300\n",
      "70000/70000 [==============================] - 1s 15us/sample - loss: 0.0117\n",
      "Epoch 66/300\n",
      "70000/70000 [==============================] - 1s 15us/sample - loss: 0.0116\n",
      "Epoch 67/300\n",
      "70000/70000 [==============================] - 1s 15us/sample - loss: 0.0116\n",
      "Epoch 68/300\n",
      "70000/70000 [==============================] - 1s 15us/sample - loss: 0.0116\n",
      "Epoch 69/300\n",
      "70000/70000 [==============================] - 1s 15us/sample - loss: 0.0115\n",
      "Epoch 70/300\n",
      "70000/70000 [==============================] - 1s 15us/sample - loss: 0.0115\n",
      "Epoch 71/300\n",
      "70000/70000 [==============================] - 1s 15us/sample - loss: 0.0114\n",
      "Epoch 72/300\n",
      "70000/70000 [==============================] - 1s 15us/sample - loss: 0.0114\n",
      "Epoch 73/300\n",
      "70000/70000 [==============================] - 1s 15us/sample - loss: 0.0114\n",
      "Epoch 74/300\n",
      "70000/70000 [==============================] - 1s 15us/sample - loss: 0.0113\n",
      "Epoch 75/300\n",
      "70000/70000 [==============================] - 1s 15us/sample - loss: 0.0113\n",
      "Epoch 76/300\n",
      "70000/70000 [==============================] - 1s 15us/sample - loss: 0.0113\n",
      "Epoch 77/300\n",
      "70000/70000 [==============================] - 1s 15us/sample - loss: 0.0112\n",
      "Epoch 78/300\n",
      "70000/70000 [==============================] - 1s 15us/sample - loss: 0.0112\n",
      "Epoch 79/300\n",
      "70000/70000 [==============================] - 1s 18us/sample - loss: 0.0112\n",
      "Epoch 80/300\n",
      "70000/70000 [==============================] - 1s 18us/sample - loss: 0.0111\n",
      "Epoch 81/300\n",
      "70000/70000 [==============================] - 1s 17us/sample - loss: 0.0111\n",
      "Epoch 82/300\n",
      "70000/70000 [==============================] - 1s 16us/sample - loss: 0.0111\n",
      "Epoch 83/300\n",
      "70000/70000 [==============================] - 1s 15us/sample - loss: 0.0110\n",
      "Epoch 84/300\n",
      "70000/70000 [==============================] - 1s 15us/sample - loss: 0.0110\n",
      "Epoch 85/300\n",
      "70000/70000 [==============================] - 1s 15us/sample - loss: 0.0110\n",
      "Epoch 86/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70000/70000 [==============================] - 1s 15us/sample - loss: 0.0109\n",
      "Epoch 87/300\n",
      "70000/70000 [==============================] - 1s 15us/sample - loss: 0.0109\n",
      "Epoch 88/300\n",
      "70000/70000 [==============================] - 1s 15us/sample - loss: 0.0109\n",
      "Epoch 89/300\n",
      "70000/70000 [==============================] - 1s 15us/sample - loss: 0.0108\n",
      "Epoch 90/300\n",
      "70000/70000 [==============================] - 1s 15us/sample - loss: 0.0108\n",
      "Epoch 91/300\n",
      "70000/70000 [==============================] - 1s 15us/sample - loss: 0.0108\n",
      "Epoch 92/300\n",
      "70000/70000 [==============================] - 1s 16us/sample - loss: 0.0108\n",
      "Epoch 93/300\n",
      "70000/70000 [==============================] - 1s 16us/sample - loss: 0.0107\n",
      "Epoch 94/300\n",
      "70000/70000 [==============================] - 1s 15us/sample - loss: 0.0107\n",
      "Epoch 95/300\n",
      "70000/70000 [==============================] - 1s 15us/sample - loss: 0.0107\n",
      "Epoch 96/300\n",
      "70000/70000 [==============================] - 1s 15us/sample - loss: 0.0106\n",
      "Epoch 97/300\n",
      "70000/70000 [==============================] - 1s 15us/sample - loss: 0.0106\n",
      "Epoch 98/300\n",
      "70000/70000 [==============================] - 1s 15us/sample - loss: 0.0106\n",
      "Epoch 99/300\n",
      "70000/70000 [==============================] - 1s 15us/sample - loss: 0.0106\n",
      "Epoch 100/300\n",
      "70000/70000 [==============================] - 1s 15us/sample - loss: 0.0106\n",
      "Epoch 101/300\n",
      "70000/70000 [==============================] - 1s 15us/sample - loss: 0.0105\n",
      "Epoch 102/300\n",
      "70000/70000 [==============================] - 1s 15us/sample - loss: 0.0105\n",
      "Epoch 103/300\n",
      "70000/70000 [==============================] - 1s 15us/sample - loss: 0.0105\n",
      "Epoch 104/300\n",
      "70000/70000 [==============================] - 1s 15us/sample - loss: 0.0104\n",
      "Epoch 105/300\n",
      "70000/70000 [==============================] - 1s 15us/sample - loss: 0.0104\n",
      "Epoch 106/300\n",
      "70000/70000 [==============================] - 1s 15us/sample - loss: 0.0104\n",
      "Epoch 107/300\n",
      "70000/70000 [==============================] - 1s 15us/sample - loss: 0.0104\n",
      "Epoch 108/300\n",
      "70000/70000 [==============================] - 1s 15us/sample - loss: 0.0104\n",
      "Epoch 109/300\n",
      "70000/70000 [==============================] - 1s 15us/sample - loss: 0.0103\n",
      "Epoch 110/300\n",
      "70000/70000 [==============================] - 1s 15us/sample - loss: 0.0103\n",
      "Epoch 111/300\n",
      "70000/70000 [==============================] - 1s 16us/sample - loss: 0.0103\n",
      "Epoch 112/300\n",
      "70000/70000 [==============================] - 1s 16us/sample - loss: 0.0103\n",
      "Epoch 113/300\n",
      "70000/70000 [==============================] - 1s 15us/sample - loss: 0.0102\n",
      "Epoch 114/300\n",
      "70000/70000 [==============================] - 1s 15us/sample - loss: 0.0102\n",
      "Epoch 115/300\n",
      "70000/70000 [==============================] - 1s 15us/sample - loss: 0.0102\n",
      "Epoch 116/300\n",
      "70000/70000 [==============================] - 1s 15us/sample - loss: 0.0102\n",
      "Epoch 117/300\n",
      "70000/70000 [==============================] - 1s 15us/sample - loss: 0.0102\n",
      "Epoch 118/300\n",
      "70000/70000 [==============================] - 1s 15us/sample - loss: 0.0101\n",
      "Epoch 119/300\n",
      "70000/70000 [==============================] - 1s 15us/sample - loss: 0.0101\n",
      "Epoch 120/300\n",
      "70000/70000 [==============================] - 1s 15us/sample - loss: 0.0101\n",
      "Epoch 121/300\n",
      "70000/70000 [==============================] - 1s 15us/sample - loss: 0.0101\n",
      "Epoch 122/300\n",
      "70000/70000 [==============================] - 1s 15us/sample - loss: 0.0101\n",
      "Epoch 123/300\n",
      "70000/70000 [==============================] - 1s 15us/sample - loss: 0.0100\n",
      "Epoch 124/300\n",
      "70000/70000 [==============================] - 1s 15us/sample - loss: 0.0100\n",
      "Epoch 125/300\n",
      "70000/70000 [==============================] - 1s 15us/sample - loss: 0.0100\n",
      "Epoch 126/300\n",
      "70000/70000 [==============================] - 1s 15us/sample - loss: 0.0100\n",
      "Epoch 127/300\n",
      "70000/70000 [==============================] - 1s 15us/sample - loss: 0.0100\n",
      "Epoch 128/300\n",
      "70000/70000 [==============================] - 1s 15us/sample - loss: 0.0099\n",
      "Epoch 129/300\n",
      "70000/70000 [==============================] - 1s 15us/sample - loss: 0.0099\n",
      "Epoch 130/300\n",
      "70000/70000 [==============================] - 1s 15us/sample - loss: 0.0099\n",
      "Epoch 131/300\n",
      "70000/70000 [==============================] - 1s 15us/sample - loss: 0.0099\n",
      "Epoch 132/300\n",
      "70000/70000 [==============================] - 1s 15us/sample - loss: 0.0099\n",
      "Epoch 133/300\n",
      "70000/70000 [==============================] - 1s 15us/sample - loss: 0.0099\n",
      "Epoch 134/300\n",
      "70000/70000 [==============================] - 1s 15us/sample - loss: 0.0098\n",
      "Epoch 135/300\n",
      "70000/70000 [==============================] - 1s 15us/sample - loss: 0.0098\n",
      "Epoch 136/300\n",
      "70000/70000 [==============================] - 1s 15us/sample - loss: 0.0098\n",
      "Epoch 137/300\n",
      "70000/70000 [==============================] - 1s 15us/sample - loss: 0.0098\n",
      "Epoch 138/300\n",
      "70000/70000 [==============================] - 1s 15us/sample - loss: 0.0098\n",
      "Epoch 139/300\n",
      "70000/70000 [==============================] - 1s 15us/sample - loss: 0.0098\n",
      "Epoch 140/300\n",
      "70000/70000 [==============================] - 1s 15us/sample - loss: 0.0097\n",
      "Epoch 141/300\n",
      "70000/70000 [==============================] - 1s 15us/sample - loss: 0.0097\n",
      "Epoch 142/300\n",
      "70000/70000 [==============================] - 1s 16us/sample - loss: 0.0097\n",
      "Epoch 143/300\n",
      "70000/70000 [==============================] - 1s 15us/sample - loss: 0.0097\n",
      "Epoch 144/300\n",
      "70000/70000 [==============================] - 1s 15us/sample - loss: 0.0097\n",
      "Epoch 145/300\n",
      "70000/70000 [==============================] - 1s 15us/sample - loss: 0.0097\n",
      "Epoch 146/300\n",
      "70000/70000 [==============================] - 1s 15us/sample - loss: 0.0096\n",
      "Epoch 147/300\n",
      "70000/70000 [==============================] - 1s 15us/sample - loss: 0.0096\n",
      "Epoch 148/300\n",
      "70000/70000 [==============================] - 1s 15us/sample - loss: 0.0096\n",
      "Epoch 149/300\n",
      "70000/70000 [==============================] - 1s 15us/sample - loss: 0.0096\n",
      "Epoch 150/300\n",
      "70000/70000 [==============================] - 1s 15us/sample - loss: 0.0096\n",
      "Epoch 151/300\n",
      "70000/70000 [==============================] - 1s 15us/sample - loss: 0.0096\n",
      "Epoch 152/300\n",
      "70000/70000 [==============================] - 1s 15us/sample - loss: 0.0096\n",
      "Epoch 153/300\n",
      "70000/70000 [==============================] - 1s 15us/sample - loss: 0.0095\n",
      "Epoch 154/300\n",
      "70000/70000 [==============================] - 1s 15us/sample - loss: 0.0095\n",
      "Epoch 155/300\n",
      "70000/70000 [==============================] - 1s 15us/sample - loss: 0.0095\n",
      "Epoch 156/300\n",
      "70000/70000 [==============================] - 1s 15us/sample - loss: 0.0095\n",
      "Epoch 157/300\n",
      "70000/70000 [==============================] - 1s 15us/sample - loss: 0.0095\n",
      "Epoch 158/300\n",
      "70000/70000 [==============================] - 1s 15us/sample - loss: 0.0095\n",
      "Epoch 159/300\n",
      "70000/70000 [==============================] - 1s 15us/sample - loss: 0.0094\n",
      "Epoch 160/300\n",
      "70000/70000 [==============================] - 1s 16us/sample - loss: 0.0094\n",
      "Epoch 161/300\n",
      "70000/70000 [==============================] - 1s 16us/sample - loss: 0.0094\n",
      "Epoch 162/300\n",
      "70000/70000 [==============================] - 1s 15us/sample - loss: 0.0094\n",
      "Epoch 163/300\n",
      "70000/70000 [==============================] - 1s 15us/sample - loss: 0.0094\n",
      "Epoch 164/300\n",
      "70000/70000 [==============================] - 1s 15us/sample - loss: 0.0094\n",
      "Epoch 165/300\n",
      "70000/70000 [==============================] - 1s 15us/sample - loss: 0.0094\n",
      "Epoch 166/300\n",
      "70000/70000 [==============================] - 1s 15us/sample - loss: 0.0094\n",
      "Epoch 167/300\n",
      "70000/70000 [==============================] - 1s 15us/sample - loss: 0.0093\n",
      "Epoch 168/300\n",
      "70000/70000 [==============================] - 1s 15us/sample - loss: 0.0093\n",
      "Epoch 169/300\n",
      "70000/70000 [==============================] - 1s 15us/sample - loss: 0.0093\n",
      "Epoch 170/300\n",
      "70000/70000 [==============================] - 1s 15us/sample - loss: 0.0093\n",
      "Epoch 171/300\n",
      "70000/70000 [==============================] - 1s 15us/sample - loss: 0.0093\n",
      "Epoch 172/300\n",
      "70000/70000 [==============================] - 1s 15us/sample - loss: 0.0093\n",
      "Epoch 173/300\n",
      "70000/70000 [==============================] - 1s 15us/sample - loss: 0.0093\n",
      "Epoch 174/300\n",
      "70000/70000 [==============================] - 1s 15us/sample - loss: 0.0093\n",
      "Epoch 175/300\n",
      "70000/70000 [==============================] - 1s 15us/sample - loss: 0.0092\n",
      "Epoch 176/300\n",
      "70000/70000 [==============================] - 1s 17us/sample - loss: 0.0092\n",
      "Epoch 177/300\n",
      "70000/70000 [==============================] - 1s 16us/sample - loss: 0.0092\n",
      "Epoch 178/300\n",
      "70000/70000 [==============================] - 1s 14us/sample - loss: 0.0092\n",
      "Epoch 179/300\n",
      "70000/70000 [==============================] - 1s 14us/sample - loss: 0.0092\n",
      "Epoch 180/300\n",
      "70000/70000 [==============================] - 1s 15us/sample - loss: 0.0092\n",
      "Epoch 181/300\n",
      "70000/70000 [==============================] - 1s 15us/sample - loss: 0.0092\n",
      "Epoch 182/300\n",
      "70000/70000 [==============================] - 1s 15us/sample - loss: 0.0092\n",
      "Epoch 183/300\n",
      "70000/70000 [==============================] - 1s 15us/sample - loss: 0.0092\n",
      "Epoch 184/300\n",
      "70000/70000 [==============================] - 1s 15us/sample - loss: 0.0091\n",
      "Epoch 185/300\n",
      "70000/70000 [==============================] - 1s 15us/sample - loss: 0.0091\n",
      "Epoch 186/300\n",
      "70000/70000 [==============================] - 1s 15us/sample - loss: 0.0091\n",
      "Epoch 187/300\n",
      "70000/70000 [==============================] - 1s 15us/sample - loss: 0.0091\n",
      "Epoch 188/300\n",
      "70000/70000 [==============================] - 1s 14us/sample - loss: 0.0091\n",
      "Epoch 189/300\n",
      "70000/70000 [==============================] - 1s 14us/sample - loss: 0.0091\n",
      "Epoch 190/300\n",
      "70000/70000 [==============================] - 1s 14us/sample - loss: 0.0091\n",
      "Epoch 191/300\n",
      "70000/70000 [==============================] - 1s 15us/sample - loss: 0.0091\n",
      "Epoch 192/300\n",
      "70000/70000 [==============================] - 1s 17us/sample - loss: 0.0091\n",
      "Epoch 193/300\n",
      "70000/70000 [==============================] - 1s 16us/sample - loss: 0.0090\n",
      "Epoch 194/300\n",
      "70000/70000 [==============================] - 1s 16us/sample - loss: 0.0090\n",
      "Epoch 195/300\n",
      "70000/70000 [==============================] - 1s 15us/sample - loss: 0.0090\n",
      "Epoch 196/300\n",
      "70000/70000 [==============================] - 1s 15us/sample - loss: 0.0090\n",
      "Epoch 197/300\n",
      "70000/70000 [==============================] - 1s 15us/sample - loss: 0.0090\n",
      "Epoch 198/300\n",
      "70000/70000 [==============================] - 1s 15us/sample - loss: 0.0090\n",
      "Epoch 199/300\n",
      "70000/70000 [==============================] - 1s 15us/sample - loss: 0.0090\n",
      "Epoch 200/300\n",
      "70000/70000 [==============================] - 1s 15us/sample - loss: 0.0090\n",
      "Epoch 201/300\n",
      "70000/70000 [==============================] - 1s 16us/sample - loss: 0.0090\n",
      "Epoch 202/300\n",
      "70000/70000 [==============================] - 1s 15us/sample - loss: 0.0090\n",
      "Epoch 203/300\n",
      "70000/70000 [==============================] - 1s 15us/sample - loss: 0.0089\n",
      "Epoch 204/300\n",
      "70000/70000 [==============================] - 1s 14us/sample - loss: 0.0089\n",
      "Epoch 205/300\n",
      "70000/70000 [==============================] - 1s 15us/sample - loss: 0.0089\n",
      "Epoch 206/300\n",
      "70000/70000 [==============================] - 1s 15us/sample - loss: 0.0089\n",
      "Epoch 207/300\n",
      "70000/70000 [==============================] - 1s 14us/sample - loss: 0.0089\n",
      "Epoch 208/300\n",
      "70000/70000 [==============================] - 1s 15us/sample - loss: 0.0089\n",
      "Epoch 209/300\n",
      "70000/70000 [==============================] - 1s 14us/sample - loss: 0.0089\n",
      "Epoch 210/300\n",
      "70000/70000 [==============================] - 1s 14us/sample - loss: 0.0089\n",
      "Epoch 211/300\n",
      "70000/70000 [==============================] - 1s 15us/sample - loss: 0.0089\n",
      "Epoch 212/300\n",
      "70000/70000 [==============================] - 1s 15us/sample - loss: 0.0089\n",
      "Epoch 213/300\n",
      "70000/70000 [==============================] - 1s 14us/sample - loss: 0.0088\n",
      "Epoch 214/300\n",
      "70000/70000 [==============================] - 1s 14us/sample - loss: 0.0088\n",
      "Epoch 215/300\n",
      "70000/70000 [==============================] - 1s 15us/sample - loss: 0.0088\n",
      "Epoch 216/300\n",
      "70000/70000 [==============================] - 1s 16us/sample - loss: 0.0088\n",
      "Epoch 217/300\n",
      "70000/70000 [==============================] - 1s 15us/sample - loss: 0.0088\n",
      "Epoch 218/300\n",
      "70000/70000 [==============================] - 1s 15us/sample - loss: 0.0088\n",
      "Epoch 219/300\n",
      "70000/70000 [==============================] - 1s 15us/sample - loss: 0.0088\n",
      "Epoch 220/300\n",
      "70000/70000 [==============================] - 1s 15us/sample - loss: 0.0088\n",
      "Epoch 221/300\n",
      "70000/70000 [==============================] - 1s 14us/sample - loss: 0.0088\n",
      "Epoch 222/300\n",
      "70000/70000 [==============================] - 1s 15us/sample - loss: 0.0088\n",
      "Epoch 223/300\n",
      "70000/70000 [==============================] - 1s 14us/sample - loss: 0.0088\n",
      "Epoch 224/300\n",
      "70000/70000 [==============================] - 1s 14us/sample - loss: 0.0087\n",
      "Epoch 225/300\n",
      "70000/70000 [==============================] - 1s 15us/sample - loss: 0.0087\n",
      "Epoch 226/300\n",
      "70000/70000 [==============================] - 1s 16us/sample - loss: 0.0087\n",
      "Epoch 227/300\n",
      "70000/70000 [==============================] - 1s 16us/sample - loss: 0.0087\n",
      "Epoch 228/300\n",
      "70000/70000 [==============================] - 1s 15us/sample - loss: 0.0087\n",
      "Epoch 229/300\n",
      "70000/70000 [==============================] - 1s 15us/sample - loss: 0.0087\n",
      "Epoch 230/300\n",
      "70000/70000 [==============================] - 1s 15us/sample - loss: 0.0087\n",
      "Epoch 231/300\n",
      "70000/70000 [==============================] - 1s 15us/sample - loss: 0.0087\n",
      "Epoch 232/300\n",
      "70000/70000 [==============================] - 1s 15us/sample - loss: 0.0087\n",
      "Epoch 233/300\n",
      "70000/70000 [==============================] - 1s 15us/sample - loss: 0.0087\n",
      "Epoch 234/300\n",
      "70000/70000 [==============================] - 1s 15us/sample - loss: 0.0087\n",
      "Epoch 235/300\n",
      "70000/70000 [==============================] - 1s 15us/sample - loss: 0.0087\n",
      "Epoch 236/300\n",
      "70000/70000 [==============================] - 1s 15us/sample - loss: 0.0087\n",
      "Epoch 237/300\n",
      "70000/70000 [==============================] - 1s 15us/sample - loss: 0.0086\n",
      "Epoch 238/300\n",
      "70000/70000 [==============================] - 1s 14us/sample - loss: 0.0086\n",
      "Epoch 239/300\n",
      "70000/70000 [==============================] - 1s 14us/sample - loss: 0.0086\n",
      "Epoch 240/300\n",
      "70000/70000 [==============================] - 1s 14us/sample - loss: 0.0086\n",
      "Epoch 241/300\n",
      "70000/70000 [==============================] - 1s 15us/sample - loss: 0.0086\n",
      "Epoch 242/300\n",
      "70000/70000 [==============================] - 1s 15us/sample - loss: 0.0086\n",
      "Epoch 243/300\n",
      "70000/70000 [==============================] - 1s 16us/sample - loss: 0.0086\n",
      "Epoch 244/300\n",
      "70000/70000 [==============================] - 1s 17us/sample - loss: 0.0086\n",
      "Epoch 245/300\n",
      "70000/70000 [==============================] - 1s 16us/sample - loss: 0.0086\n",
      "Epoch 246/300\n",
      "70000/70000 [==============================] - 1s 16us/sample - loss: 0.0086\n",
      "Epoch 247/300\n",
      "70000/70000 [==============================] - 1s 16us/sample - loss: 0.0085\n",
      "Epoch 248/300\n",
      "70000/70000 [==============================] - 1s 16us/sample - loss: 0.0085\n",
      "Epoch 249/300\n",
      "70000/70000 [==============================] - 1s 16us/sample - loss: 0.0085\n",
      "Epoch 250/300\n",
      "70000/70000 [==============================] - 1s 16us/sample - loss: 0.0085\n",
      "Epoch 251/300\n",
      "70000/70000 [==============================] - 1s 15us/sample - loss: 0.0085\n",
      "Epoch 252/300\n",
      "70000/70000 [==============================] - 1s 15us/sample - loss: 0.0085\n",
      "Epoch 253/300\n",
      "70000/70000 [==============================] - 1s 15us/sample - loss: 0.0085\n",
      "Epoch 254/300\n",
      "70000/70000 [==============================] - 1s 15us/sample - loss: 0.0085\n",
      "Epoch 255/300\n",
      "70000/70000 [==============================] - 1s 15us/sample - loss: 0.0085\n",
      "Epoch 256/300\n",
      "70000/70000 [==============================] - 1s 14us/sample - loss: 0.0085\n",
      "Epoch 257/300\n",
      "70000/70000 [==============================] - 1s 14us/sample - loss: 0.0085\n",
      "Epoch 258/300\n",
      "70000/70000 [==============================] - 1s 15us/sample - loss: 0.0085\n",
      "Epoch 259/300\n",
      "70000/70000 [==============================] - 1s 15us/sample - loss: 0.0085\n",
      "Epoch 260/300\n",
      "70000/70000 [==============================] - 1s 15us/sample - loss: 0.0085\n",
      "Epoch 261/300\n",
      "70000/70000 [==============================] - 1s 15us/sample - loss: 0.0085\n",
      "Epoch 262/300\n",
      "70000/70000 [==============================] - 1s 15us/sample - loss: 0.0084\n",
      "Epoch 263/300\n",
      "70000/70000 [==============================] - 1s 14us/sample - loss: 0.0084\n",
      "Epoch 264/300\n",
      "70000/70000 [==============================] - 1s 15us/sample - loss: 0.0084\n",
      "Epoch 265/300\n",
      "70000/70000 [==============================] - 1s 15us/sample - loss: 0.0084\n",
      "Epoch 266/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70000/70000 [==============================] - 1s 15us/sample - loss: 0.0084\n",
      "Epoch 267/300\n",
      "70000/70000 [==============================] - 1s 15us/sample - loss: 0.0084\n",
      "Epoch 268/300\n",
      "70000/70000 [==============================] - 1s 15us/sample - loss: 0.0084\n",
      "Epoch 269/300\n",
      "70000/70000 [==============================] - 1s 15us/sample - loss: 0.0084\n",
      "Epoch 270/300\n",
      "70000/70000 [==============================] - 1s 15us/sample - loss: 0.0084\n",
      "Epoch 271/300\n",
      "70000/70000 [==============================] - 1s 15us/sample - loss: 0.0084\n",
      "Epoch 272/300\n",
      "70000/70000 [==============================] - 1s 15us/sample - loss: 0.0084\n",
      "Epoch 273/300\n",
      "70000/70000 [==============================] - 1s 16us/sample - loss: 0.0084\n",
      "Epoch 274/300\n",
      "70000/70000 [==============================] - 1s 17us/sample - loss: 0.0084\n",
      "Epoch 275/300\n",
      "70000/70000 [==============================] - 1s 17us/sample - loss: 0.0084\n",
      "Epoch 276/300\n",
      "70000/70000 [==============================] - 1s 16us/sample - loss: 0.0084\n",
      "Epoch 277/300\n",
      "70000/70000 [==============================] - 1s 16us/sample - loss: 0.0083\n",
      "Epoch 278/300\n",
      "70000/70000 [==============================] - 1s 15us/sample - loss: 0.0083\n",
      "Epoch 279/300\n",
      "70000/70000 [==============================] - 1s 15us/sample - loss: 0.0083\n",
      "Epoch 280/300\n",
      "70000/70000 [==============================] - 1s 14us/sample - loss: 0.0083\n",
      "Epoch 281/300\n",
      "70000/70000 [==============================] - 1s 15us/sample - loss: 0.0083\n",
      "Epoch 282/300\n",
      "70000/70000 [==============================] - 1s 15us/sample - loss: 0.0083\n",
      "Epoch 283/300\n",
      "70000/70000 [==============================] - 1s 15us/sample - loss: 0.0083\n",
      "Epoch 284/300\n",
      "70000/70000 [==============================] - 1s 15us/sample - loss: 0.0083\n",
      "Epoch 285/300\n",
      "70000/70000 [==============================] - 1s 16us/sample - loss: 0.0083\n",
      "Epoch 286/300\n",
      "70000/70000 [==============================] - 1s 17us/sample - loss: 0.0083\n",
      "Epoch 287/300\n",
      "70000/70000 [==============================] - 1s 16us/sample - loss: 0.0083\n",
      "Epoch 288/300\n",
      "70000/70000 [==============================] - 1s 16us/sample - loss: 0.0083\n",
      "Epoch 289/300\n",
      "70000/70000 [==============================] - 1s 16us/sample - loss: 0.0083\n",
      "Epoch 290/300\n",
      "70000/70000 [==============================] - 1s 16us/sample - loss: 0.0083\n",
      "Epoch 291/300\n",
      "70000/70000 [==============================] - 1s 16us/sample - loss: 0.0083\n",
      "Epoch 292/300\n",
      "70000/70000 [==============================] - 1s 15us/sample - loss: 0.0082\n",
      "Epoch 293/300\n",
      "70000/70000 [==============================] - 1s 15us/sample - loss: 0.0082\n",
      "Epoch 294/300\n",
      "70000/70000 [==============================] - 1s 15us/sample - loss: 0.0082\n",
      "Epoch 295/300\n",
      "70000/70000 [==============================] - 1s 15us/sample - loss: 0.0082\n",
      "Epoch 296/300\n",
      "70000/70000 [==============================] - 1s 16us/sample - loss: 0.0082\n",
      "Epoch 297/300\n",
      "70000/70000 [==============================] - 1s 17us/sample - loss: 0.0082\n",
      "Epoch 298/300\n",
      "70000/70000 [==============================] - 1s 17us/sample - loss: 0.0082\n",
      "Epoch 299/300\n",
      "70000/70000 [==============================] - 1s 17us/sample - loss: 0.0082\n",
      "Epoch 300/300\n",
      "70000/70000 [==============================] - 1s 17us/sample - loss: 0.0082\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Unable to create file (unable to open file: name = './results/ae_weights.h5', errno = 2, error message = 'No such file or directory', flags = 13, o_flags = 242)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-16dd3768a35e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mautoencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpretrain_optimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'mse'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mautoencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpretrain_epochs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#, callbacks=cb)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mautoencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_dir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/ae_weights.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/network.py\u001b[0m in \u001b[0;36msave_weights\u001b[0;34m(self, filepath, overwrite, save_format)\u001b[0m\n\u001b[1;32m   1412\u001b[0m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1413\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msave_format\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'h5'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1414\u001b[0;31m       \u001b[0;32mwith\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1415\u001b[0m         \u001b[0msaving\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_weights_to_hdf5_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1416\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, **kwds)\u001b[0m\n\u001b[1;32m    392\u001b[0m                 fid = make_fid(name, mode, userblock_size,\n\u001b[1;32m    393\u001b[0m                                \u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfcpl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmake_fcpl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrack_order\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack_order\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 394\u001b[0;31m                                swmr=swmr)\n\u001b[0m\u001b[1;32m    395\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36mmake_fid\u001b[0;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_EXCL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfcpl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfcpl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_TRUNC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfcpl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfcpl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'a'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0;31m# Open in append mode (read/write).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/h5f.pyx\u001b[0m in \u001b[0;36mh5py.h5f.create\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: Unable to create file (unable to open file: name = './results/ae_weights.h5', errno = 2, error message = 'No such file or directory', flags = 13, o_flags = 242)"
     ]
    }
   ],
   "source": [
    "autoencoder.compile(optimizer=pretrain_optimizer, loss='mse')\n",
    "autoencoder.fit(x, x, batch_size=batch_size, epochs=pretrain_epochs) #, callbacks=cb)\n",
    "autoencoder.save_weights(save_dir + '/ae_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.save_weights(save_dir + '/ae_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.load_weights(save_dir + '/ae_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClusteringLayer(Layer):\n",
    "    \"\"\"\n",
    "    Clustering layer converts input sample (feature) to soft label, i.e. a vector that represents the probability of the\n",
    "    sample belonging to each cluster. The probability is calculated with student's t-distribution.\n",
    "\n",
    "    # Example\n",
    "    ```\n",
    "        model.add(ClusteringLayer(n_clusters=10))\n",
    "    ```\n",
    "    # Arguments\n",
    "        n_clusters: number of clusters.\n",
    "        weights: list of Numpy array with shape `(n_clusters, n_features)` witch represents the initial cluster centers.\n",
    "        alpha: degrees of freedom parameter in Student's t-distribution. Default to 1.0.\n",
    "    # Input shape\n",
    "        2D tensor with shape: `(n_samples, n_features)`.\n",
    "    # Output shape\n",
    "        2D tensor with shape: `(n_samples, n_clusters)`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_clusters, weights=None, alpha=1.0, **kwargs):\n",
    "        if 'input_shape' not in kwargs and 'input_dim' in kwargs:\n",
    "            kwargs['input_shape'] = (kwargs.pop('input_dim'),)\n",
    "        super(ClusteringLayer, self).__init__(**kwargs)\n",
    "        self.n_clusters = n_clusters\n",
    "        self.alpha = alpha\n",
    "        self.initial_weights = weights\n",
    "        self.input_spec = InputSpec(ndim=2)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 2\n",
    "        input_dim = input_shape[1]\n",
    "        self.input_spec = InputSpec(dtype=K.floatx(), shape=(None, input_dim))\n",
    "        self.clusters = self.add_weight(name=\"clustering\", shape = (int(self.n_clusters), int(input_dim)), initializer='glorot_uniform',trainable=True)\n",
    "        if self.initial_weights is not None:\n",
    "            self.set_weights(self.initial_weights)\n",
    "            del self.initial_weights\n",
    "        self.built = True\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        \"\"\" student t-distribution, as same as used in t-SNE algorithm.\n",
    "         Measure the similarity between embedded point z_i and centroid _j.\n",
    "                 q_ij = 1/(1+dist(x_i, _j)^2), then normalize it.\n",
    "                 q_ij can be interpreted as the probability of assigning sample i to cluster j.\n",
    "                 (i.e., a soft assignment)\n",
    "        Arguments:\n",
    "            inputs: the variable containing data, shape=(n_samples, n_features)\n",
    "        Return:\n",
    "            q: student's t-distribution, or soft labels for each sample. shape=(n_samples, n_clusters)\n",
    "        \"\"\"\n",
    "        q = 1.0 / (1.0 + (K.sum(K.square(K.expand_dims(inputs, axis=1) - self.clusters), axis=2) / self.alpha))\n",
    "        q **= (self.alpha + 1.0) / 2.0\n",
    "        q = K.transpose(K.transpose(q) / K.sum(q, axis=1)) # Make sure each sample's 10 values add up to 1.\n",
    "        return q\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        assert input_shape and len(input_shape) == 2\n",
    "        return input_shape[0], self.n_clusters\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'n_clusters': self.n_clusters}\n",
    "        base_config = super(ClusteringLayer, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering_layer = ClusteringLayer(n_clusters, name='clustering')(encoder.output)\n",
    "model = Model(inputs=encoder.input, outputs=clustering_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=SGD(0.01, 0.9), loss='kld')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=n_clusters, n_init=20)\n",
    "y_pred = kmeans.fit_predict(encoder.predict(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_last = np.copy(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_layer(name='clustering').set_weights([kmeans.cluster_centers_])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# computing an auxiliary target distribution\n",
    "def target_distribution(q):\n",
    "    weight = q ** 2 / q.sum(0)\n",
    "    return (weight.T / weight.sum(1)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = 0\n",
    "index = 0\n",
    "maxiter = 8000\n",
    "update_interval = 140\n",
    "index_array = np.arange(x.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "tol = 0.001 # tolerance threshold to stop training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/king/anaconda3/lib/python3.7/site-packages/sklearn/metrics/cluster/supervised.py:844: FutureWarning: The behavior of NMI will change in version 0.22. To match the behavior of 'v_measure_score', NMI will use average_method='arithmetic' by default.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0: acc = 0.91256, nmi = 0.82029, ari = 0.81521  ; loss= 0\n",
      "Iter 140: acc = 0.92089, nmi = 0.83093, ari = 0.83332  ; loss= 0.04619\n",
      "Iter 280: acc = 0.93281, nmi = 0.85022, ari = 0.85777  ; loss= 0.15459\n",
      "Iter 420: acc = 0.94059, nmi = 0.86294, ari = 0.87380  ; loss= 0.2047\n",
      "Iter 560: acc = 0.94543, nmi = 0.87193, ari = 0.88383  ; loss= 0.21731\n",
      "Iter 700: acc = 0.94887, nmi = 0.87776, ari = 0.89094  ; loss= 0.21237\n",
      "Iter 840: acc = 0.95151, nmi = 0.88263, ari = 0.89639  ; loss= 0.20751\n",
      "Iter 980: acc = 0.95300, nmi = 0.88542, ari = 0.89950  ; loss= 0.20571\n",
      "Iter 1120: acc = 0.95427, nmi = 0.88787, ari = 0.90218  ; loss= 0.19575\n",
      "Iter 1260: acc = 0.95507, nmi = 0.88937, ari = 0.90386  ; loss= 0.19596\n",
      "Iter 1400: acc = 0.95580, nmi = 0.89090, ari = 0.90536  ; loss= 0.19036\n",
      "Iter 1540: acc = 0.95640, nmi = 0.89227, ari = 0.90663  ; loss= 0.17978\n",
      "Iter 1680: acc = 0.95674, nmi = 0.89285, ari = 0.90733  ; loss= 0.17421\n",
      "Iter 1820: acc = 0.95733, nmi = 0.89427, ari = 0.90855  ; loss= 0.18425\n",
      "Iter 1960: acc = 0.95774, nmi = 0.89500, ari = 0.90942  ; loss= 0.16846\n",
      "Iter 2100: acc = 0.95790, nmi = 0.89541, ari = 0.90976  ; loss= 0.16906\n",
      "Iter 2240: acc = 0.95794, nmi = 0.89553, ari = 0.90987  ; loss= 0.16751\n",
      "Iter 2380: acc = 0.95820, nmi = 0.89608, ari = 0.91040  ; loss= 0.16453\n",
      "Iter 2520: acc = 0.95827, nmi = 0.89628, ari = 0.91057  ; loss= 0.16146\n",
      "Iter 2660: acc = 0.95844, nmi = 0.89663, ari = 0.91089  ; loss= 0.16543\n",
      "Iter 2800: acc = 0.95839, nmi = 0.89664, ari = 0.91080  ; loss= 0.15586\n",
      "Iter 2940: acc = 0.95831, nmi = 0.89659, ari = 0.91063  ; loss= 0.14609\n",
      "Iter 3080: acc = 0.95843, nmi = 0.89690, ari = 0.91087  ; loss= 0.14972\n",
      "Iter 3220: acc = 0.95836, nmi = 0.89681, ari = 0.91072  ; loss= 0.14246\n",
      "Iter 3360: acc = 0.95863, nmi = 0.89743, ari = 0.91129  ; loss= 0.13889\n",
      "Iter 3500: acc = 0.95846, nmi = 0.89705, ari = 0.91092  ; loss= 0.14358\n",
      "Iter 3640: acc = 0.95873, nmi = 0.89766, ari = 0.91149  ; loss= 0.13782\n",
      "Iter 3780: acc = 0.95866, nmi = 0.89758, ari = 0.91136  ; loss= 0.1318\n",
      "Iter 3920: acc = 0.95877, nmi = 0.89785, ari = 0.91159  ; loss= 0.13492\n",
      "Iter 4060: acc = 0.95877, nmi = 0.89788, ari = 0.91160  ; loss= 0.134\n",
      "Iter 4200: acc = 0.95881, nmi = 0.89806, ari = 0.91169  ; loss= 0.12594\n",
      "Iter 4340: acc = 0.95877, nmi = 0.89795, ari = 0.91160  ; loss= 0.12439\n",
      "Iter 4480: acc = 0.95871, nmi = 0.89793, ari = 0.91149  ; loss= 0.12397\n",
      "Iter 4620: acc = 0.95881, nmi = 0.89811, ari = 0.91171  ; loss= 0.12766\n",
      "Iter 4760: acc = 0.95881, nmi = 0.89807, ari = 0.91169  ; loss= 0.12519\n",
      "Iter 4900: acc = 0.95890, nmi = 0.89828, ari = 0.91188  ; loss= 0.12719\n",
      "Iter 5040: acc = 0.95861, nmi = 0.89767, ari = 0.91127  ; loss= 0.12038\n",
      "Iter 5180: acc = 0.95873, nmi = 0.89794, ari = 0.91151  ; loss= 0.12305\n",
      "Iter 5320: acc = 0.95841, nmi = 0.89720, ari = 0.91087  ; loss= 0.12388\n",
      "Iter 5460: acc = 0.95884, nmi = 0.89805, ari = 0.91174  ; loss= 0.12055\n",
      "Iter 5600: acc = 0.95856, nmi = 0.89737, ari = 0.91116  ; loss= 0.11824\n",
      "Iter 5740: acc = 0.95871, nmi = 0.89778, ari = 0.91148  ; loss= 0.10798\n",
      "Iter 5880: acc = 0.95874, nmi = 0.89789, ari = 0.91156  ; loss= 0.11334\n",
      "Iter 6020: acc = 0.95871, nmi = 0.89771, ari = 0.91148  ; loss= 0.11096\n",
      "Iter 6160: acc = 0.95873, nmi = 0.89784, ari = 0.91153  ; loss= 0.10786\n",
      "Iter 6300: acc = 0.95874, nmi = 0.89782, ari = 0.91157  ; loss= 0.10351\n",
      "Iter 6440: acc = 0.95861, nmi = 0.89769, ari = 0.91130  ; loss= 0.10528\n",
      "Iter 6580: acc = 0.95880, nmi = 0.89797, ari = 0.91169  ; loss= 0.10789\n",
      "Iter 6720: acc = 0.95866, nmi = 0.89777, ari = 0.91138  ; loss= 0.10828\n",
      "Iter 6860: acc = 0.95883, nmi = 0.89806, ari = 0.91174  ; loss= 0.10251\n",
      "Iter 7000: acc = 0.95873, nmi = 0.89781, ari = 0.91153  ; loss= 0.10534\n",
      "delta_label  0.0009 < tol  0.001\n",
      "Reached tolerance threshold. Stopping training.\n"
     ]
    }
   ],
   "source": [
    "for ite in range(int(maxiter)):\n",
    "    if ite % update_interval == 0:\n",
    "        q = model.predict(x, verbose=0)\n",
    "        p = target_distribution(q)  # update the auxiliary target distribution p\n",
    "\n",
    "        # evaluate the clustering performance\n",
    "        y_pred = q.argmax(1)\n",
    "        if y is not None:\n",
    "            acc = np.round(metrics.acc(y, y_pred), 5)\n",
    "            nmi = np.round(metrics.nmi(y, y_pred), 5)\n",
    "            ari = np.round(metrics.ari(y, y_pred), 5)\n",
    "            loss = np.round(loss, 5)\n",
    "            print('Iter %d: acc = %.5f, nmi = %.5f, ari = %.5f' % (ite, acc, nmi, ari), ' ; loss=', loss)\n",
    "\n",
    "        # check stop criterion - model convergence\n",
    "        delta_label = np.sum(y_pred != y_pred_last).astype(np.float32) / y_pred.shape[0]\n",
    "        y_pred_last = np.copy(y_pred)\n",
    "        if ite > 0 and delta_label < tol:\n",
    "            print('delta_label ', delta_label, '< tol ', tol)\n",
    "            print('Reached tolerance threshold. Stopping training.')\n",
    "            break\n",
    "    idx = index_array[index * batch_size: min((index+1) * batch_size, x.shape[0])]\n",
    "    loss = model.train_on_batch(x=x[idx], y=p[idx])\n",
    "    index = index + 1 if (index + 1) * batch_size <= x.shape[0] else 0\n",
    "\n",
    "model.save_weights(save_dir + '/DEC_model_final.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(save_dir + '/DEC_model_final.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc = 0.95873, nmi = 0.89781, ari = 0.91153  ; loss= 0.10534\n"
     ]
    }
   ],
   "source": [
    "# Eval.\n",
    "q = model.predict(x, verbose=0)\n",
    "p = target_distribution(q)  # update the auxiliary target distribution p\n",
    "\n",
    "# evaluate the clustering performance\n",
    "y_pred = q.argmax(1)\n",
    "if y is not None:\n",
    "    acc = np.round(metrics.acc(y, y_pred), 5)\n",
    "    nmi = np.round(metrics.nmi(y, y_pred), 5)\n",
    "    ari = np.round(metrics.ari(y, y_pred), 5)\n",
    "    loss = np.round(loss, 5)\n",
    "    print('Acc = %.5f, nmi = %.5f, ari = %.5f' % (acc, nmi, ari), ' ; loss=', loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1600x1400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import sklearn.metrics\n",
    "import matplotlib.pyplot as plt\n",
    "sns.set(font_scale=3)\n",
    "confusion_matrix = sklearn.metrics.confusion_matrix(y, y_pred)\n",
    "\n",
    "plt.figure(figsize=(16, 14))\n",
    "sns.heatmap(confusion_matrix, annot=True, fmt=\"d\", annot_kws={\"size\": 20});\n",
    "plt.title(\"Confusion matrix\", fontsize=30)\n",
    "plt.ylabel('True label', fontsize=25)\n",
    "plt.xlabel('Clustering label', fontsize=25)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9587285714285714"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.utils.linear_assignment_ import linear_assignment\n",
    "\n",
    "y_true = y.astype(np.int64)\n",
    "D = max(y_pred.max(), y_true.max()) + 1\n",
    "w = np.zeros((D, D), dtype=np.int64)\n",
    "# Confusion matrix.\n",
    "for i in range(y_pred.size):\n",
    "    w[y_pred[i], y_true[i]] += 1\n",
    "ind = linear_assignment(-w)\n",
    "\n",
    "sum([w[i, j] for i, j in ind]) * 1.0 / y_pred.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6795,    2,   32,    3,    4,   28,   31,    8,   18,   28],\n",
       "       [   1,   29,   52,   53,    8,    3,    0, 6978,    9,  127],\n",
       "       [  47,   45,  107,   56,    9,   18,   11,    8, 6434,   55],\n",
       "       [   6,   91, 6674,  113,   16,   19,    5,   97,   41,    4],\n",
       "       [   5,   13,   51,    1, 6633,    8,   13,   45,   18,   91],\n",
       "       [   0, 7645,    7,    1,    4,    3,   10,   28,   30,   10],\n",
       "       [   4,   35,   43, 6820,    0,   91,    0,    8,   98,  124],\n",
       "       [   6,    2,    2,   76,    0, 6073,  242,    3,   74,   20],\n",
       "       [  28,    4,   15,    2,   34,   47, 6564,    0,   12,    4],\n",
       "       [  11,   11,    7,   16,  116,   23,    0,  118,   91, 6495]])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0],\n",
       "       [1, 7],\n",
       "       [2, 8],\n",
       "       [3, 2],\n",
       "       [4, 4],\n",
       "       [5, 1],\n",
       "       [6, 3],\n",
       "       [7, 5],\n",
       "       [8, 6],\n",
       "       [9, 9]])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 7, 8, 2, 4, 1, 3, 5, 6, 9])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w.argmax(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
